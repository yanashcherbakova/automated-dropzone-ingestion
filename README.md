# üåÄ Automated Dropzone Ingestion
### This project provides a reusable template for automated, local-first data processing and delivery to Amazon S3, with multi-stage logging and support for both local execution and Docker-based runs.

![Architecture diagram](pics/dropzone_ingestion_schema.png)

## Project Features

- CSV ‚Üí Parquet transformation (customizable)
- Synthetic data generator for testing
- Local raw / processed data layers
- Two-queue architecture: processing queue and upload queue
- File system‚Äìbased ingestion with event-driven detection
- Safe file ingestion via temporary `.tmp` files to prevent reading partially written files
- Failure handling with local persistence in failed/
- Rescan-based reconciliation for incoming data and failed S3 uploads
- Layered logging architecture with dedicated loggers:
  - `dropzone.reading` ‚Äî file ingestion and filesystem events
  - `dropzone.processing` ‚Äî CSV validation, transformation, and Parquet generation
  - `dropzone.uploader` ‚Äî S3 upload lifecycle, retries, and failure handling

# Project Structure üìÅ
```text
automated-dropzone-ingestion/
‚îú‚îÄ‚îÄ aws/
‚îÇ   ‚îî‚îÄ‚îÄ s3_utils.py                 # S3 key generation and upload logic (keys, partitions)

‚îú‚îÄ‚îÄ incoming_watcher/
‚îÇ   ‚îú‚îÄ‚îÄ watcher.py                  # Filesystem watcher (incoming files)
‚îÇ   ‚îî‚îÄ‚îÄ process_worker.py           # Processing + rescan (CSV ‚Üí DataFrame ‚Üí Parquet)

‚îú‚îÄ‚îÄ s3_upload/
‚îÇ   ‚îú‚îÄ‚îÄ s3_parquet_uploader.py      # Processed Parquet watcher + thread startup
‚îÇ   ‚îî‚îÄ‚îÄ uploader_worker.py          # Upload queue, workers, and rescan logic

‚îú‚îÄ‚îÄ log_shipper/
‚îÇ   ‚îî‚îÄ‚îÄ log_shipper.py              # Log discovery and upload to S3

‚îú‚îÄ‚îÄ synth_data/
‚îÇ   ‚îú‚îÄ‚îÄ gen_synth_data.py           # Synthetic CSV generator (test-only)
‚îÇ   ‚îî‚îÄ‚îÄ values.py                   # Reference values for synthetic data

‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ queue_utils.py              # Shared queue utilities

‚îú‚îÄ‚îÄ file_storage/
‚îÇ   ‚îú‚îÄ‚îÄ incoming/                   # Incoming raw CSV files
‚îÇ   ‚îú‚îÄ‚îÄ processed/                  # Generated Parquet files
‚îÇ   ‚îú‚îÄ‚îÄ failed/                     # Failed files by stage
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ read/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transform/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ upload/
‚îÇ   ‚îî‚îÄ‚îÄ failed_logs/                # Failed log upload artifacts

‚îú‚îÄ‚îÄ logging_config.py               # Logging setup and rotation
‚îú‚îÄ‚îÄ docker-compose.yml              # Multi-container local pipeline
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .env.docker
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .dockerignore
‚îî‚îÄ‚îÄ README.md
```

# How to test üë∂

```
git clone https://github.com/yanashcherbakova/automated-dropzone-ingestion.git
cd automated-dropzone-ingestion
```

## Option A --> Run with Docker Compose üê≥

1. Create `.env.docker`

```
AWS_REGION=(us-east-1)
S3_BUCKET=(your-bucket)
S3_PREFIX=(data)

INCOMING_DIR=/app/file_storage/incoming
PROCESSED_DIR=/app/file_storage/processed

FAILED_DIR_READ=/app/file_storage/failed/read
FAILED_DIR_TRANSFORM=/app/file_storage/failed/transform
FAILED_DIR_UPLOAD=/app/file_storage/failed/upload

WATCHDOG_POLLING=1
```
2. `docker compose up --build`

This command starts three Docker containers, each running a dedicated Python service:
- **CSV generator (test-only)**
    - Generates synthetic transaction CSV files and drops them into the incoming folder.
    - This container exists only to validate that the end-to-end pipeline works.
- **Processor**
    - Watches the incoming folder, loads CSV files into `pandas DataFrames`, performs cleaning and validation, converts them to Parquet, stores them in the processing directory, and removes the original CSV files.
- **Uploader**
    - Detects newly created `Parquet files`, enqueues them, and uploads them to `Amazon S3` using the configured bucket and prefix.

![Docker](pics/docker_up.png)
3. Follow logs
```
docker compose logs -f processor
docker compose logs -f uploader
```
![Docker](pics/processor_logs.png)
![Docker](pics/uploader_logs.png)

## Option B --> Run locally (no Docker)

1. Create and activate venv
```
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```
2. Create `.env`
```
AWS_REGION=(us-east-1)
S3_BUCKET=(your-bucket)
S3_PREFIX=(data)

INCOMING_DIR=./file_storage/incoming
PROCESSED_DIR=./file_storage/processed

FAILED_DIR_READ=./file_storage/failed/read
FAILED_DIR_TRANSFORM=./file_storage/failed/transform
FAILED_DIR_UPLOAD=./file_storage/failed/upload

WATCHDOG_POLLING=0
```
3. Start services (3 terminals)
- **Terminal 1 - generator *(optional)***
    - `python3 -m synth_data.gen_synth_data`
- **Terminal 2 - processor**
    - `python3 -m incoming_watcher.watcher`
- **Terminal 3 - uploader**
    - `python3 -m s3_upload.s3_parquet_uploader`

4. Validate
- A new CSV should appear in `file_storage/incoming/`
- Processor should create Parquet in `file_storage/processed/`
- Uploader should upload Parquet to S3 and then remove it locally (or move to `failed/upload` on errors)

Logs example / Generator

![Local](pics/local_logs_generator.png)

Logs example / Processor

![Local](pics/local_logs_process.png)

### Finally

### üß© Check S3 Bucket:

![Local](pics/s3.png)

Each file follows the same end-to-end path through the pipeline.

![Custom](pics/one_item_pipeline.png)

Different processing stages can be identified by the **container names** that emit the logs.  
The same separation is also visible in the unified log stream, which is grouped into three logical domains:

- `dropzone.reading` ‚Äî file ingestion and filesystem events
- `dropzone.processing` ‚Äî CSV validation, transformation, and Parquet generation
- `dropzone.uploader` ‚Äî S3 upload lifecycle, retries, and failure handling

# Fast customization üèÇ What to customize?

![Custom](pics/custom/custom.png)

## 1. Data Flow (Data Source)

![Custom](pics/custom/custom_data_flow.png)

- This project generates a **synthetic transaction table** used to simulate a real data flow.
- In a production pipeline, this component would be replaced by an actual data source (for example, an upstream service, external provider, or ingestion endpoint).

![Custom](pics/custom/test_csv.png)

The data is delivered in **CSV format**.

### üß© Check: `incoming_watcher/watcher.py`

**‚ùóImportant:** at the moment, the pipeline ingests data from CSV and loads it into a pandas DataFrame. All downstream processing (validation, cleaning, transformations) is implemented at the DataFrame level.

If a different input format is required (e.g. Parquet, JSON, Avro), the downstream processing logic must be adapted accordingly, rather than only swapping the input file.

## 2. Process worker

![Custom](pics/custom/custom_process_worker.png)

As described earlier, incoming CSV files are loaded into pandas DataFrames, then passed through cleaning and validation steps, for example:

- column name normalization
- standardizing currency codes / payment method names
- basic sanity checks (nulls, types, ranges, etc.)

The resulting DataFrame is then written to Parquet and moved to: `folder_sorage/processing`

### üß© Check: `incoming_watcher/process_worker.py`

### Customizing the processing logic

You can fully replace the processing logic inside: `def process_file(file_path):`

**‚ùóImportant:** downstream components detect and enqueue Parquet files from the processing folder.
If you change where Parquet files are written, their naming, or their schema, review the downstream logic here:

- `s3_upload/s3_parquet_uploader.py`
- `s3_upload/uploader_worker.py`

### Customizing S3 key / partitioning

![custom](pics/custom/default_key.png)

S3 object naming and partitioning is defined in: 
- `aws/s3_utils.py` (s3_key)

Current partitioning format: **year=YYYY/month=MM/day=DD**

## 3. LOGS
![Custom](pics/custom/custom_logs.png)

### üß© Check: 
- `logging_config.py`
    - change log rotation settings (max size / number of backups)
    - adjust log format (timestamps, logger name, level, message)
- `incoming_watcher/watcher.py`
    - starts a background thread responsible for shipping logs to S3
- `log_shipper/log_shipper.py`
    - implements the actual log shipping logic

## Rescan cases

### Case 1 - Files stuck in the incoming folder üì•‚è∏Ô∏è

In this test scenario, a batch of CSV files remains unprocessed in the incoming folder.

This can happen in the following situations:
- ‚è≥ The data source delivered files while the processor container (or the processor script in local mode) was not running
- üê≥ In a Docker setup, the data-producing container started earlier than the processor
(`depends_on` in Docker Compose helps with startup order but doesn`t provide a strict guarantee)
- üëÄ The filesystem watcher missed the file creation event

As a result, files remain unnoticed in the incoming directory:

![rescan](pics/rescan/stack_in_incoming.png)

To handle this, a **periodic rescan** mechanism is used. 
By default, **every 60 seconds**, the rescan checks the incoming folder and enqueues all eligible files. File validation and queue deduplication are handled inside:
- `incoming_watcher/process_worker.py`

This ensures that previously missed files are picked up and re-introduced into the pipeline

In the logs, it is clearly visible that the file was enqueued by the rescan process, not by the watcher:

![rescan](pics/rescan/incoming_rescan.png)

### Case 2 - Network connectivity loss during S3 upload üîå‚¨áÔ∏è

Network outages are usually unpredictable.
In this test case, the failure is intentionally reproduced to validate retry and rescan logic üôÇ

![rescan](pics/rescan/hihi.png)

The S3 delivery logic performs multiple retry attempts.
If all retries fail, the file is moved to:

`file_storage/failed/upload`

In this scenario, the folder contains **fully processed Parquet files** ‚Äî
i.e. files from the processed layer that were not delivered to S3:

![rescan](pics/rescan/stack_in_failed_upload.png)

The rescan mechanism periodically checks this directory and selects only Parquet files.
These files are then enqueued for another upload attempt.

Again, the logs explicitly indicate that the file was queued by the **rescan logic**:

![rescan](pics/rescan/no_connection_logs.png)